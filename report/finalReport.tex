\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\renewcommand{\arraystretch}{1.4}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi
\setcounter{page}{1}
\begin{document}

%%%%%%%%% TITLE
\title{Hierarchical Multiclass Object Classification }

\author{Fereshte Khani\\ 
Massachusetts Institute of Technology\\
{\tt\small fereshte@mit.edu}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Anurag Mukkara\\
Massachusetts Institute of Technology\\
{\tt\small anurag\_m@mit.edu}
}

% TODO
% spell check
% bold text for variables


\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}

Human can use similarity between objects in order to recognize rare objects. They also make many abstract concepts when they see some objects very often. Interestingly, a large part of brain is associated with common classes like faces rather than rare objects like Ostrich.
In our work we want to propose a model that has four mentioned characteristics. 1. Use more resources for categories that have many examples and less resources for categories that have few examples. 2. Has the ability recognize objects that it has never seen before, but can be made by combining previously seen objects. 3. Objects with few examples can borrow statistical strength from similar objects with many examples. 4. Models that have lot of sharing between model parameters of different classes should be favored.

\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}

\section{Previous work}


\section{Hierarchical Classification Model}

\subsection{Learning Independent Classification Models}

Consider a classification problem where we observe a dataset $D$ of labelled  training examples. 
Each example belongs belongs to one $K$ classes(eg. 40 object categories). The dataset $D$
has $n_{k}$ labeled examples for for each class $k  \in {1,2,....,K} $. We $x_{i}^{(k)} \in R^{D} $ 
be the input feature vector of length D for the $i^{th}$ training example of class $k$ and 
$y_{i}^{(k)}$ be the corresponding class label. $ y_{i}^{k} = 1 $ indicates that the $i^{th}$ training example 
 is a positive example for class $K$ and  $ y_{i}^{k} = -1 $ indicates that the $i^{th}$ training example 
 is a negative example for class $K$.
 
 For a binary classification problem we can use a simple logistic regression model. In particular, 
 for each class $k$, the probability of a positive instance can be modeled as: 
 \begin{equation}
 p(y_{i}^{(k)} = 1 | \beta^{(k)} )  = \frac { \exp( \beta^{(k)^{T}}  x_{i}^{(k)})  }{  1 + \exp(\beta^{(k)^{T}}  x_{i}^{(k)} ) } 
 \end{equation}
  where $k$ ranges over the $K$ classes and $ \beta^{k}  \in R^{D} $ is the unknown model parameter of length D 
  for class k. 
  In addition, we place a zero-mean spherical Gaussian prior over each model parameter  $  \beta^{k} $:
  \begin{equation}
	p(\beta)  = \prod_{k=1}^{K} p(\beta^{(k)}) = \prod_{k=1}^{K} N(0, \frac{1}{\lambda})
  \end{equation}
 where $N(\mu, \Sigma)$ indicates a Gaussian probability distribution with mean $\mu$ and covariance $\Sigma$.
 Here, $\lambda$ is the prior parameter.  With this modeling framework, the log posterior distribution of the unknown 
 parameters can be expresses as 
 \begin{equation}
	\log p(\beta | D) \propto \sum_{k=1}^{K} \Big[ \, \ \sum_{i=1}^{n_{k}} \log p(y_{i}^{(k)} | x_{i}^{(k)}, \beta^{(k)}) \ + \ \log p(\beta^{(k)}) \ \Big] \,
\end{equation}
 Maximizing the log posterior(or finding the MAP estimate ) over the unknown model parameters $\beta$  is  equivalent to 
 minimizing the probabilistic loss function with quadratic regularization terms:
 \begin{equation}
 	E = \sum_{k=1}^{K}  \Big[ \, \ \sum_{i=1}^{n_{k}} Loss(y_{i}^{(k)}, x_{i}^{(k)}, \beta^{(k)}) + \frac{\lambda}{2} \| \beta^{(k)}) \|^{2} \ \Big] \,
 \end{equation} 
 where the probabilistic loss function is given by:
 \begin{equation}
 %Loss(y_{i}^{(k)},  x_{i}^{(k)},  \beta^{(k)})  =  - \sum_{j \in {-1,1} } I\{y_{i}^{(k)}=j\} \log p(y_{i}^{(k)}=j | x_{i}^{(k)},\beta^{(k)})
 Loss(y,  x,  \beta)  =  - \sum_{j \in {-1,1} } I\{y=j\} \log p(y=j | x,\beta)
\end{equation} 
One important property of the loss function is that it is convex and hence can be efficiently optimized
using standard convex optimization solvers.
 
 The main drawback of this formulation is that all the $K$ classes are treated as unrelated entities and hence 
 do not share any model parameters. In fact, it is observed that the objective function decomposes into 
 $K$ sub-problems, each of which is a binary classification problem that can be trained independently 
 using the training data for that particular class.
 
 
 \subsection{Learning a Hierarchical Classification Model}
 In our framework, a hierarchical tree structure is used to model sharing of parameters between 
 visually related classes. Each node in the tree has a model parameter, a vector of length $D$ 
 associated with it.  Let $ \theta = \{ \theta_{1}, \theta_{2},.....,\theta_{n}\} $ denote the model parameters 
 of all the nodes in the tree, where $n$ is the number of nodes starting from the root node to the all the leaf nodes.
 The leaf nodes correspond to the $K$ object categories or classes. The parameters for a particular class $k$
 is the sum of the parameters of all the nodes, say $i_{1}, i_{2}, ..., i_{l} $, along the path from the leaf node
 corresponding to that class up until the root node of the tree:
 \begin{equation} \label{beta-theta-rel}
 \beta^{(k)} = \theta_{i_{1}} +  \theta_{i_{2}} + ...... + \theta_{i_{l}}
  \end{equation} 
 We place zero-mean spherical Gaussian priors on the parameters of each node. However, the 
 covariance matrix is dependent on the level of a particular node in the tree i.e all the nodes at
 a particular level have the same prior parameters.
 \begin{equation}
 p(\theta_{i})  = N(0,\frac{1}{\lambda_{l}}I) 
\end{equation}   
where node $i$ is at level $l$ in the tree. The root node has level 0 and the level increases as 
we move towards the leaf nodes.

With this modification in the way the model parameters for classes are related,  the hierarchical 
learning problem can be formulated as minimizing the log posterior or equivalently the loss function 
with respect to $\beta$:
\begin{equation} \label{hier-loss}
\begin{split}
E = Loss(Y,X,\beta) + \frac{\lambda_{0}}{2} \|\theta_{0} \|^{2} +
\sum_{i=1}^{k_{1}} \frac{\lambda_{1}}{2} \|\theta_{i1} \|^{2} & \\
+ \sum_{i=1}^{k_{2}} \frac{\lambda_{2}}{2} \|\theta_{i2} \|^{2} +
...... +
\sum_{i=1}^{k_{l}} \frac{\lambda_{l}}{2} \|\theta_{il} \|^{2} &
\end{split}
\end{equation} 
 where $Loss(Y,X,\beta)$ is the loss function as explained in the previous section
 but with the additional constraint that $\beta$ satisfies  equation \ref{beta-theta-rel} 
 according to the tree structure. Here there are $k_{j}$ nodes in level $j$ and 
 $\theta_{1j}, \theta_{2j},.....,\theta_{k_{j}j}$ are the parameters of nodes in level $j$.

\subsection{Learning the tree structure}
If we are given the tree structure that indicates the relation between different classes, then the 
learning algorithm only needs to optimize for the loss in equation \ref{hier-loss}. However, as 
discussed in \cite{Ruslan}, learning the tree structure dynamically based on the input classes and training data
could lead to better results in object classification. We adopt the algorithm used in \cite{Ruslan} to multilevel trees
and fine-grained partitioning. In \cite{Ruslan}, the author use a fixed number of levels i.e 3 in the tree and use
a non-parametric Chinese Restaurant Process Prior(CRP) to model the position of a class in the tree. Although this simple
CRP prior allows one to have unknown and potentially unbounded number of groups of object classes, this is can only be
used for two-level trees. For trees which have a fixed number of levels more than 3, one could use a nested CRP prior \cite{nestedCRP}.
Since this approach has the drawback of limiting the tree to a fixed number of levels, we use a simple extension(modified-CRP) of the
CRP prior used in \cite{Ruslan} to allow the flexibility of  having different depths at different parts of the tree.    

A new class could go into one of the existing super-categories or create a new super-category. A new super-category 
can be attached to any node which doesn't have any children that are leaf nodes. That is, we allow the parent of a new
super-category to be any node that is not itself a super-category that has leaf nodes, which correspond to classes, as children.
By following the procedure in \cite{Ruslan},  for each possible position of the new class we calculate a likelihood term and 
modified-CRP prior term. The position of the new class will be the one that maximizes the sum of likelihood term and  prior term. 

Let $i$ denote the node number of the leaf node corresponding to the new class.   
To calculate the likelihood at a possible position in the tree, we find the model parameter $\theta_{i}$ of the leaf node, 
that maximizes the log likelihood of the training data at that position. This likelihood will depend on the position in the tree, when 
we choose a position it means that we include the parameters of all the nodes that are ancestors of the leaf node $i$ at that position. 
If the position corresponds to a category that is visually dissimilar to the new class, then the best we can with $\theta_{i}$ with still 
result in a lower likelihood term. If the the category is visually similar to the new class, we can achieve a far better fit and the likelihood
term goes up. We have observed that the maximum of the likelihood over the model parameter $\theta_{i}$ is greatly affected by the position
 we choose for the leaf node. In addition to the likelihood term, the modified-CRP prior term favors super-categories that have a large number of 
 leaf nodes or class under them. The modified-CRP prior is given by:
 
 \begin{displaymath}
 p(z=j) = \left \{
	    \begin{array}{lr}
 		\frac{m_{j}}{n+\gamma*m} & : \text{$j$ is old} \\
		\frac{\gamma}{n+\gamma*m} & : \text{$j$ is new}
	   \end{array}
	  \right.
 \end{displaymath}  
where $n$ is the total number of leaf nodes, $m_{j}$
is the number of leaf nodes under super-category $j$,
$\gamma$ is the concentration parameter that controls the probability of creating a new super-category,
$m$ is the total number of positions of new super-categories in the tree. $m$ is equal to number of nodes
in the tree that have only super-categories as children. In other words, $m$ is the cardinality of the set that 
includes the root node as well all nodes that are not leaf nodes or parents of leaf nodes. It can be easily verified that 
this defines a valid probability distribution over the choice $z$ of possible positions in the tree where the new class can be placed.
{\small
\bibliographystyle{ieee}
\bibliography{finalReport}
}

\end{document}
